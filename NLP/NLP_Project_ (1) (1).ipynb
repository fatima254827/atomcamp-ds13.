{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Student Name: Fatima Nawab"
      ],
      "metadata": {
        "id": "t18RZx9Jk60L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Natural Language Processing | DS & AI Cohort 13**\n",
        "**Objective**\n",
        "\n",
        "Understand and apply core NLP techniques - stemming, lemmatization, N-grams,\n",
        "vectorization methods, and Naive Bayes classification to build and evaluate a complete text classification pipeline.\n",
        "\n",
        "Your goal is to transform raw text into meaningful representations and\n",
        "use a machine learning model to perform sentiment classification.\n"
      ],
      "metadata": {
        "id": "SHDZqvCOYwcx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Importing necessary libraries"
      ],
      "metadata": {
        "id": "JqpUpizOh3iK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "N80ayToA2a0n"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n"
      ],
      "metadata": {
        "id": "6IsdhRUoV-6g"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvklWMUs2oS-",
        "outputId": "ad871ac0-f07a-43c8-f49c-14bd393df9e2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1. Stemming and Lemmatization\n"
      ],
      "metadata": {
        "id": "aGmPuaN4YZLe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the IMDB dataset, using the 'python' engine for better handling of parsing errors\n",
        "df = pd.read_csv('/content/IMDB Dataset.csv', engine='python')\n",
        "\n",
        "# Display the first 5 rows\n",
        "print(\"First 5 rows of the DataFrame:\")\n",
        "print(df.head())\n",
        "\n",
        "# Print the shape of the DataFrame\n",
        "print(\"\\nShape of the DataFrame:\")\n",
        "print(df.shape)\n",
        "\n",
        "# Display concise summary of the DataFrame\n",
        "print(\"\\nConcise summary of the DataFrame:\")\n",
        "df.info()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cpx-wzRE3opL",
        "outputId": "38fa779a-a452-4998-a0ad-aeb236a91abb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 rows of the DataFrame:\n",
            "                                              review sentiment\n",
            "0  One of the other reviewers has mentioned that ...  positive\n",
            "1  A wonderful little production. <br /><br />The...  positive\n",
            "2  I thought this was a wonderful way to spend ti...  positive\n",
            "3  Basically there's a family where a little boy ...  negative\n",
            "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
            "\n",
            "Shape of the DataFrame:\n",
            "(50000, 2)\n",
            "\n",
            "Concise summary of the DataFrame:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 50000 entries, 0 to 49999\n",
            "Data columns (total 2 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   review     50000 non-null  object\n",
            " 1   sentiment  50000 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 781.4+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['sentiment'].value_counts()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "yMCQssJDGlxb",
        "outputId": "10f4920a-3e06-4e37-8557-59797bb94e1f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sentiment\n",
              "positive    25000\n",
              "negative    25000\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sentiment</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>positive</th>\n",
              "      <td>25000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>negative</th>\n",
              "      <td>25000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n"
      ],
      "metadata": {
        "id": "3Otng8G_Qe_9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Text Preprocessing\n",
        "###1.1. Define Stopwords, Stemmer & Lemmatizer"
      ],
      "metadata": {
        "id": "60JMBrGRRUdF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "porter_stemmer = PorterStemmer()\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "def perform_stemming(token_list):\n",
        "    return [porter_stemmer.stem(token) for token in token_list]\n",
        "\n",
        "def perform_lemmatization(token_list):\n",
        "    return [wordnet_lemmatizer.lemmatize(token) for token in token_list]\n"
      ],
      "metadata": {
        "id": "D-WdqsY7WNvU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.2. Text Cleaning Function"
      ],
      "metadata": {
        "id": "fQPXbgLURp4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_review_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "\n",
        "    # Remove punctuation and special characters\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
        "\n",
        "    # Tokenize text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove English stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Return cleaned text as a single string\n",
        "    return \" \".join(tokens)\n",
        "\n"
      ],
      "metadata": {
        "id": "_bOaRC2WRfcu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.3. Apply Text Cleaning"
      ],
      "metadata": {
        "id": "GMteytytRxnk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Starting text preprocessing...\")\n",
        "df['clean_review'] = df['review'].apply(clean_review_text)\n",
        "print(\"Text preprocessing completed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opFCO2xES9_j",
        "outputId": "856e0c08-559d-4539-e600-5096b7615998"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting text preprocessing...\n",
            "Text preprocessing completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.4. Tokenization Function"
      ],
      "metadata": {
        "id": "npUJRyAbUVUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_review(text):\n",
        "    return word_tokenize(text)\n"
      ],
      "metadata": {
        "id": "A3f79ncmUV4r"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tokenizing cleaned reviews...\")\n",
        "df['review_tokens'] = df['clean_review'].apply(tokenize_review)\n",
        "print(\"Tokenization completed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NaXO4t4AUeEt",
        "outputId": "b892f369-8ddb-4a05-8e9c-d1b9757762cd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing cleaned reviews...\n",
            "Tokenization completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.5. Define Stemming and Lemmatization Functions"
      ],
      "metadata": {
        "id": "PPzXBSXRUcGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_stemming(token_list):\n",
        "    return [porter_stemmer.stem(token) for token in token_list]\n",
        "\n",
        "def perform_lemmatization(token_list):\n",
        "    return [wordnet_lemmatizer.lemmatize(token) for token in token_list]\n"
      ],
      "metadata": {
        "id": "tivc7N6vUkwj"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.6. Apply Stemming & Lemmatization"
      ],
      "metadata": {
        "id": "TzvOh1tbUnl8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Applying stemming...\")\n",
        "df['review_stemmed'] = df['review_tokens'].apply(perform_stemming)\n",
        "print(\"Stemming completed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUP_b0ezUrwS",
        "outputId": "eadfc643-5bab-4926-8c9c-dc9ddc38780f"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applying stemming...\n",
            "Stemming completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Applying lemmatization...\")\n",
        "df['review_lemmatized'] = df['review_tokens'].apply(perform_lemmatization)\n",
        "print(\"Lemmatization completed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMdWj6yaUvPU",
        "outputId": "0e2e5acf-9d02-4cac-9240-bb1eef4c24b5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applying lemmatization...\n",
            "Lemmatization completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.7. Compare Results on Sample Review"
      ],
      "metadata": {
        "id": "xhjnguNsUx5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_row = 1\n",
        "\n",
        "print(\"\\n--- Text Processing Comparison ---\")\n",
        "print(\"Original Review:\\n\", df['review'].iloc[sample_row])\n",
        "print(\"\\nTokenized Review:\\n\", df['review_tokens'].iloc[sample_row])\n",
        "print(\"\\nStemmed Review:\\n\", df['review_stemmed'].iloc[sample_row])\n",
        "print(\"\\nLemmatized Review:\\n\", df['review_lemmatized'].iloc[sample_row])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CLIHtecU03Y",
        "outputId": "399704e4-f824-4248-b7c7-b7ec1e4633b7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Text Processing Comparison ---\n",
            "Original Review:\n",
            " A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. <br /><br />The actors are extremely well chosen- Michael Sheen not only \"has got all the polari\" but he has all the voices down pat too! You can truly see the seamless editing guided by the references to Williams' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. A masterful production about one of the great master's of comedy and his life. <br /><br />The realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional 'dream' techniques remains solid then disappears. It plays on our knowledge and our senses, particularly with the scenes concerning Orton and Halliwell and the sets (particularly of their flat with Halliwell's murals decorating every surface) are terribly well done.\n",
            "\n",
            "Tokenized Review:\n",
            " ['wonderful', 'little', 'production', 'filming', 'technique', 'unassuming', 'oldtimebbc', 'fashion', 'gives', 'comforting', 'sometimes', 'discomforting', 'sense', 'realism', 'entire', 'piece', 'actors', 'extremely', 'well', 'chosen', 'michael', 'sheen', 'got', 'polari', 'voices', 'pat', 'truly', 'see', 'seamless', 'editing', 'guided', 'references', 'williams', 'diary', 'entries', 'well', 'worth', 'watching', 'terrificly', 'written', 'performed', 'piece', 'masterful', 'production', 'one', 'great', 'masters', 'comedy', 'life', 'realism', 'really', 'comes', 'home', 'little', 'things', 'fantasy', 'guard', 'rather', 'use', 'traditional', 'dream', 'techniques', 'remains', 'solid', 'disappears', 'plays', 'knowledge', 'senses', 'particularly', 'scenes', 'concerning', 'orton', 'halliwell', 'sets', 'particularly', 'flat', 'halliwells', 'murals', 'decorating', 'every', 'surface', 'terribly', 'well', 'done']\n",
            "\n",
            "Stemmed Review:\n",
            " ['wonder', 'littl', 'product', 'film', 'techniqu', 'unassum', 'oldtimebbc', 'fashion', 'give', 'comfort', 'sometim', 'discomfort', 'sens', 'realism', 'entir', 'piec', 'actor', 'extrem', 'well', 'chosen', 'michael', 'sheen', 'got', 'polari', 'voic', 'pat', 'truli', 'see', 'seamless', 'edit', 'guid', 'refer', 'william', 'diari', 'entri', 'well', 'worth', 'watch', 'terrificli', 'written', 'perform', 'piec', 'master', 'product', 'one', 'great', 'master', 'comedi', 'life', 'realism', 'realli', 'come', 'home', 'littl', 'thing', 'fantasi', 'guard', 'rather', 'use', 'tradit', 'dream', 'techniqu', 'remain', 'solid', 'disappear', 'play', 'knowledg', 'sens', 'particularli', 'scene', 'concern', 'orton', 'halliwel', 'set', 'particularli', 'flat', 'halliwel', 'mural', 'decor', 'everi', 'surfac', 'terribl', 'well', 'done']\n",
            "\n",
            "Lemmatized Review:\n",
            " ['wonderful', 'little', 'production', 'filming', 'technique', 'unassuming', 'oldtimebbc', 'fashion', 'give', 'comforting', 'sometimes', 'discomforting', 'sense', 'realism', 'entire', 'piece', 'actor', 'extremely', 'well', 'chosen', 'michael', 'sheen', 'got', 'polari', 'voice', 'pat', 'truly', 'see', 'seamless', 'editing', 'guided', 'reference', 'williams', 'diary', 'entry', 'well', 'worth', 'watching', 'terrificly', 'written', 'performed', 'piece', 'masterful', 'production', 'one', 'great', 'master', 'comedy', 'life', 'realism', 'really', 'come', 'home', 'little', 'thing', 'fantasy', 'guard', 'rather', 'use', 'traditional', 'dream', 'technique', 'remains', 'solid', 'disappears', 'play', 'knowledge', 'sens', 'particularly', 'scene', 'concerning', 'orton', 'halliwell', 'set', 'particularly', 'flat', 'halliwells', 'mural', 'decorating', 'every', 'surface', 'terribly', 'well', 'done']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.8. Analysis: Stemming vs Lemmatization (Markdown Section)\n",
        "\n",
        "Stemming is a rule-based technique that trims words to their root form. While it is computationally efficient, it may generate words that do not exist in the dictionary.\n",
        "\n",
        "Lemmatization uses vocabulary and morphological analysis to return meaningful base forms of words. Although slower than stemming, it preserves semantic correctness.\n",
        "\n",
        "In this experiment, lemmatized text retained better readability and interpretability, making it more suitable for downstream NLP tasks such as sentiment analysis."
      ],
      "metadata": {
        "id": "CWJ46KZfU-OO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. N-gram Language Modeling (Unigrams, Bigrams, Trigrams)"
      ],
      "metadata": {
        "id": "z-LmGH7zZuVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "_NrXAbzSZw-E"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.1. Function to Create N-grams"
      ],
      "metadata": {
        "id": "wqRWzULqZzaT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_ngrams(token_list, n_value):\n",
        "    \"\"\"\n",
        "    Generates n-grams from a list of tokens.\n",
        "    \"\"\"\n",
        "    return list(ngrams(token_list, n_value))\n"
      ],
      "metadata": {
        "id": "sAS8hpYJZ2WC"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.2. Generate Unigrams, Bigrams & Trigrams for Reviews"
      ],
      "metadata": {
        "id": "Ccdg1ccNZ5Qs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Creating unigram representations...\")\n",
        "df['review_unigrams'] = df['review_tokens'].apply(lambda tokens: create_ngrams(tokens, 1))\n",
        "\n",
        "print(\"Creating bigram representations...\")\n",
        "df['review_bigrams'] = df['review_tokens'].apply(lambda tokens: create_ngrams(tokens, 2))\n",
        "\n",
        "print(\"Creating trigram representations...\")\n",
        "df['review_trigrams'] = df['review_tokens'].apply(lambda tokens: create_ngrams(tokens, 3))\n",
        "\n",
        "print(\"N-gram creation completed successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYaGnhMZc5IW",
        "outputId": "6b63b18b-4fe6-47ce-f43f-c794bcbaab3b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating unigram representations...\n",
            "Creating bigram representations...\n",
            "Creating trigram representations...\n",
            "N-gram creation completed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.3. Examine N-grams for a Sample Review"
      ],
      "metadata": {
        "id": "QL_hTYsqdFWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_index = 0\n",
        "\n",
        "example_text = df['clean_review'].iloc[example_index]\n",
        "example_unigrams = df['review_unigrams'].iloc[example_index]\n",
        "example_bigrams = df['review_bigrams'].iloc[example_index]\n",
        "example_trigrams = df['review_trigrams'].iloc[example_index]\n",
        "\n",
        "print(f\"\\n--- N-gram Analysis for Review at Index {example_index} ---\")\n",
        "print(\"\\nCleaned Review Text:\\n\", example_text)\n",
        "print(\"\\nSample Unigrams (first 10):\\n\", example_unigrams[:10])\n",
        "print(\"\\nSample Bigrams (first 10):\\n\", example_bigrams[:10])\n",
        "print(\"\\nSample Trigrams (first 10):\\n\", example_trigrams[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWp8uSyZd2jU",
        "outputId": "f56c44aa-bc09-4f19-e6da-cb9f10c8ec3e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- N-gram Analysis for Review at Index 0 ---\n",
            "\n",
            "Cleaned Review Text:\n",
            " one reviewers mentioned watching 1 oz episode youll hooked right exactly happened methe first thing struck oz brutality unflinching scenes violence set right word go trust show faint hearted timid show pulls punches regards drugs sex violence hardcore classic use wordit called oz nickname given oswald maximum security state penitentary focuses mainly emerald city experimental section prison cells glass fronts face inwards privacy high agenda em city home manyaryans muslims gangstas latinos christians italians irish moreso scuffles death stares dodgy dealings shady agreements never far awayi would say main appeal show due fact goes shows wouldnt dare forget pretty pictures painted mainstream audiences forget charm forget romanceoz doesnt mess around first episode ever saw struck nasty surreal couldnt say ready watched developed taste oz got accustomed high levels graphic violence violence injustice crooked guards wholl sold nickel inmates wholl kill order get away well mannered middle class inmates turned prison bitches due lack street skills prison experience watching oz may become comfortable uncomfortable viewingthats get touch darker side\n",
            "\n",
            "Sample Unigrams (first 10):\n",
            " [('one',), ('reviewers',), ('mentioned',), ('watching',), ('1',), ('oz',), ('episode',), ('youll',), ('hooked',), ('right',)]\n",
            "\n",
            "Sample Bigrams (first 10):\n",
            " [('one', 'reviewers'), ('reviewers', 'mentioned'), ('mentioned', 'watching'), ('watching', '1'), ('1', 'oz'), ('oz', 'episode'), ('episode', 'youll'), ('youll', 'hooked'), ('hooked', 'right'), ('right', 'exactly')]\n",
            "\n",
            "Sample Trigrams (first 10):\n",
            " [('one', 'reviewers', 'mentioned'), ('reviewers', 'mentioned', 'watching'), ('mentioned', 'watching', '1'), ('watching', '1', 'oz'), ('1', 'oz', 'episode'), ('oz', 'episode', 'youll'), ('episode', 'youll', 'hooked'), ('youll', 'hooked', 'right'), ('hooked', 'right', 'exactly'), ('right', 'exactly', 'happened')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.4. Define Counters for all N-grams and display frequencies and probabilities"
      ],
      "metadata": {
        "id": "nG6Pq6MdfDkE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "sample_index = 0\n",
        "# Sample review N-grams\n",
        "sample_unigrams = df['review_unigrams'].iloc[sample_index]\n",
        "sample_bigrams = df['review_bigrams'].iloc[sample_index]\n",
        "sample_trigrams = df['review_trigrams'].iloc[sample_index]\n",
        "\n",
        "# Count frequencies\n",
        "unigram_counts = Counter(sample_unigrams)\n",
        "bigram_counts = Counter(sample_bigrams)\n",
        "trigram_counts = Counter(sample_trigrams)\n",
        "\n",
        "# Display frequencies and probabilities\n",
        "print(\"\\n--- Sample N-gram Frequencies and Probabilities ---\")\n",
        "\n",
        "print(\"\\nUnigrams:\")\n",
        "total_unigrams = len(sample_unigrams)\n",
        "for gram, count in unigram_counts.most_common():\n",
        "    print(f\"  {gram}: Count={count}, Probability={count/total_unigrams:.4f}\")\n",
        "\n",
        "print(\"\\nBigrams:\")\n",
        "total_bigrams = len(sample_bigrams)\n",
        "if total_bigrams > 0:\n",
        "    for gram, count in bigram_counts.most_common():\n",
        "        print(f\"  {gram}: Count={count}, Probability={count/total_bigrams:.4f}\")\n",
        "else:\n",
        "    print(\"  No bigrams generated.\")\n",
        "\n",
        "print(\"\\nTrigrams:\")\n",
        "total_trigrams = len(sample_trigrams)\n",
        "if total_trigrams > 0:\n",
        "    for gram, count in trigram_counts.most_common():\n",
        "        print(f\"  {gram}: Count={count}, Probability={count/total_trigrams:.4f}\")\n",
        "else:\n",
        "    print(\"  No trigrams generated.\")\n",
        "\n",
        "print(\"\\nN-gram probability calculation complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6GSx6ergkTy",
        "outputId": "ad4ad1de-39f5-41c6-9e00-de499ec6690a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Sample N-gram Frequencies and Probabilities ---\n",
            "\n",
            "Unigrams:\n",
            "  ('oz',): Count=5, Probability=0.0298\n",
            "  ('violence',): Count=4, Probability=0.0238\n",
            "  ('show',): Count=3, Probability=0.0179\n",
            "  ('prison',): Count=3, Probability=0.0179\n",
            "  ('forget',): Count=3, Probability=0.0179\n",
            "  ('watching',): Count=2, Probability=0.0119\n",
            "  ('episode',): Count=2, Probability=0.0119\n",
            "  ('right',): Count=2, Probability=0.0119\n",
            "  ('first',): Count=2, Probability=0.0119\n",
            "  ('struck',): Count=2, Probability=0.0119\n",
            "  ('city',): Count=2, Probability=0.0119\n",
            "  ('high',): Count=2, Probability=0.0119\n",
            "  ('say',): Count=2, Probability=0.0119\n",
            "  ('due',): Count=2, Probability=0.0119\n",
            "  ('wholl',): Count=2, Probability=0.0119\n",
            "  ('inmates',): Count=2, Probability=0.0119\n",
            "  ('get',): Count=2, Probability=0.0119\n",
            "  ('one',): Count=1, Probability=0.0060\n",
            "  ('reviewers',): Count=1, Probability=0.0060\n",
            "  ('mentioned',): Count=1, Probability=0.0060\n",
            "  ('1',): Count=1, Probability=0.0060\n",
            "  ('youll',): Count=1, Probability=0.0060\n",
            "  ('hooked',): Count=1, Probability=0.0060\n",
            "  ('exactly',): Count=1, Probability=0.0060\n",
            "  ('happened',): Count=1, Probability=0.0060\n",
            "  ('methe',): Count=1, Probability=0.0060\n",
            "  ('thing',): Count=1, Probability=0.0060\n",
            "  ('brutality',): Count=1, Probability=0.0060\n",
            "  ('unflinching',): Count=1, Probability=0.0060\n",
            "  ('scenes',): Count=1, Probability=0.0060\n",
            "  ('set',): Count=1, Probability=0.0060\n",
            "  ('word',): Count=1, Probability=0.0060\n",
            "  ('go',): Count=1, Probability=0.0060\n",
            "  ('trust',): Count=1, Probability=0.0060\n",
            "  ('faint',): Count=1, Probability=0.0060\n",
            "  ('hearted',): Count=1, Probability=0.0060\n",
            "  ('timid',): Count=1, Probability=0.0060\n",
            "  ('pulls',): Count=1, Probability=0.0060\n",
            "  ('punches',): Count=1, Probability=0.0060\n",
            "  ('regards',): Count=1, Probability=0.0060\n",
            "  ('drugs',): Count=1, Probability=0.0060\n",
            "  ('sex',): Count=1, Probability=0.0060\n",
            "  ('hardcore',): Count=1, Probability=0.0060\n",
            "  ('classic',): Count=1, Probability=0.0060\n",
            "  ('use',): Count=1, Probability=0.0060\n",
            "  ('wordit',): Count=1, Probability=0.0060\n",
            "  ('called',): Count=1, Probability=0.0060\n",
            "  ('nickname',): Count=1, Probability=0.0060\n",
            "  ('given',): Count=1, Probability=0.0060\n",
            "  ('oswald',): Count=1, Probability=0.0060\n",
            "  ('maximum',): Count=1, Probability=0.0060\n",
            "  ('security',): Count=1, Probability=0.0060\n",
            "  ('state',): Count=1, Probability=0.0060\n",
            "  ('penitentary',): Count=1, Probability=0.0060\n",
            "  ('focuses',): Count=1, Probability=0.0060\n",
            "  ('mainly',): Count=1, Probability=0.0060\n",
            "  ('emerald',): Count=1, Probability=0.0060\n",
            "  ('experimental',): Count=1, Probability=0.0060\n",
            "  ('section',): Count=1, Probability=0.0060\n",
            "  ('cells',): Count=1, Probability=0.0060\n",
            "  ('glass',): Count=1, Probability=0.0060\n",
            "  ('fronts',): Count=1, Probability=0.0060\n",
            "  ('face',): Count=1, Probability=0.0060\n",
            "  ('inwards',): Count=1, Probability=0.0060\n",
            "  ('privacy',): Count=1, Probability=0.0060\n",
            "  ('agenda',): Count=1, Probability=0.0060\n",
            "  ('em',): Count=1, Probability=0.0060\n",
            "  ('home',): Count=1, Probability=0.0060\n",
            "  ('manyaryans',): Count=1, Probability=0.0060\n",
            "  ('muslims',): Count=1, Probability=0.0060\n",
            "  ('gangstas',): Count=1, Probability=0.0060\n",
            "  ('latinos',): Count=1, Probability=0.0060\n",
            "  ('christians',): Count=1, Probability=0.0060\n",
            "  ('italians',): Count=1, Probability=0.0060\n",
            "  ('irish',): Count=1, Probability=0.0060\n",
            "  ('moreso',): Count=1, Probability=0.0060\n",
            "  ('scuffles',): Count=1, Probability=0.0060\n",
            "  ('death',): Count=1, Probability=0.0060\n",
            "  ('stares',): Count=1, Probability=0.0060\n",
            "  ('dodgy',): Count=1, Probability=0.0060\n",
            "  ('dealings',): Count=1, Probability=0.0060\n",
            "  ('shady',): Count=1, Probability=0.0060\n",
            "  ('agreements',): Count=1, Probability=0.0060\n",
            "  ('never',): Count=1, Probability=0.0060\n",
            "  ('far',): Count=1, Probability=0.0060\n",
            "  ('awayi',): Count=1, Probability=0.0060\n",
            "  ('would',): Count=1, Probability=0.0060\n",
            "  ('main',): Count=1, Probability=0.0060\n",
            "  ('appeal',): Count=1, Probability=0.0060\n",
            "  ('fact',): Count=1, Probability=0.0060\n",
            "  ('goes',): Count=1, Probability=0.0060\n",
            "  ('shows',): Count=1, Probability=0.0060\n",
            "  ('wouldnt',): Count=1, Probability=0.0060\n",
            "  ('dare',): Count=1, Probability=0.0060\n",
            "  ('pretty',): Count=1, Probability=0.0060\n",
            "  ('pictures',): Count=1, Probability=0.0060\n",
            "  ('painted',): Count=1, Probability=0.0060\n",
            "  ('mainstream',): Count=1, Probability=0.0060\n",
            "  ('audiences',): Count=1, Probability=0.0060\n",
            "  ('charm',): Count=1, Probability=0.0060\n",
            "  ('romanceoz',): Count=1, Probability=0.0060\n",
            "  ('doesnt',): Count=1, Probability=0.0060\n",
            "  ('mess',): Count=1, Probability=0.0060\n",
            "  ('around',): Count=1, Probability=0.0060\n",
            "  ('ever',): Count=1, Probability=0.0060\n",
            "  ('saw',): Count=1, Probability=0.0060\n",
            "  ('nasty',): Count=1, Probability=0.0060\n",
            "  ('surreal',): Count=1, Probability=0.0060\n",
            "  ('couldnt',): Count=1, Probability=0.0060\n",
            "  ('ready',): Count=1, Probability=0.0060\n",
            "  ('watched',): Count=1, Probability=0.0060\n",
            "  ('developed',): Count=1, Probability=0.0060\n",
            "  ('taste',): Count=1, Probability=0.0060\n",
            "  ('got',): Count=1, Probability=0.0060\n",
            "  ('accustomed',): Count=1, Probability=0.0060\n",
            "  ('levels',): Count=1, Probability=0.0060\n",
            "  ('graphic',): Count=1, Probability=0.0060\n",
            "  ('injustice',): Count=1, Probability=0.0060\n",
            "  ('crooked',): Count=1, Probability=0.0060\n",
            "  ('guards',): Count=1, Probability=0.0060\n",
            "  ('sold',): Count=1, Probability=0.0060\n",
            "  ('nickel',): Count=1, Probability=0.0060\n",
            "  ('kill',): Count=1, Probability=0.0060\n",
            "  ('order',): Count=1, Probability=0.0060\n",
            "  ('away',): Count=1, Probability=0.0060\n",
            "  ('well',): Count=1, Probability=0.0060\n",
            "  ('mannered',): Count=1, Probability=0.0060\n",
            "  ('middle',): Count=1, Probability=0.0060\n",
            "  ('class',): Count=1, Probability=0.0060\n",
            "  ('turned',): Count=1, Probability=0.0060\n",
            "  ('bitches',): Count=1, Probability=0.0060\n",
            "  ('lack',): Count=1, Probability=0.0060\n",
            "  ('street',): Count=1, Probability=0.0060\n",
            "  ('skills',): Count=1, Probability=0.0060\n",
            "  ('experience',): Count=1, Probability=0.0060\n",
            "  ('may',): Count=1, Probability=0.0060\n",
            "  ('become',): Count=1, Probability=0.0060\n",
            "  ('comfortable',): Count=1, Probability=0.0060\n",
            "  ('uncomfortable',): Count=1, Probability=0.0060\n",
            "  ('viewingthats',): Count=1, Probability=0.0060\n",
            "  ('touch',): Count=1, Probability=0.0060\n",
            "  ('darker',): Count=1, Probability=0.0060\n",
            "  ('side',): Count=1, Probability=0.0060\n",
            "\n",
            "Bigrams:\n",
            "  ('one', 'reviewers'): Count=1, Probability=0.0060\n",
            "  ('reviewers', 'mentioned'): Count=1, Probability=0.0060\n",
            "  ('mentioned', 'watching'): Count=1, Probability=0.0060\n",
            "  ('watching', '1'): Count=1, Probability=0.0060\n",
            "  ('1', 'oz'): Count=1, Probability=0.0060\n",
            "  ('oz', 'episode'): Count=1, Probability=0.0060\n",
            "  ('episode', 'youll'): Count=1, Probability=0.0060\n",
            "  ('youll', 'hooked'): Count=1, Probability=0.0060\n",
            "  ('hooked', 'right'): Count=1, Probability=0.0060\n",
            "  ('right', 'exactly'): Count=1, Probability=0.0060\n",
            "  ('exactly', 'happened'): Count=1, Probability=0.0060\n",
            "  ('happened', 'methe'): Count=1, Probability=0.0060\n",
            "  ('methe', 'first'): Count=1, Probability=0.0060\n",
            "  ('first', 'thing'): Count=1, Probability=0.0060\n",
            "  ('thing', 'struck'): Count=1, Probability=0.0060\n",
            "  ('struck', 'oz'): Count=1, Probability=0.0060\n",
            "  ('oz', 'brutality'): Count=1, Probability=0.0060\n",
            "  ('brutality', 'unflinching'): Count=1, Probability=0.0060\n",
            "  ('unflinching', 'scenes'): Count=1, Probability=0.0060\n",
            "  ('scenes', 'violence'): Count=1, Probability=0.0060\n",
            "  ('violence', 'set'): Count=1, Probability=0.0060\n",
            "  ('set', 'right'): Count=1, Probability=0.0060\n",
            "  ('right', 'word'): Count=1, Probability=0.0060\n",
            "  ('word', 'go'): Count=1, Probability=0.0060\n",
            "  ('go', 'trust'): Count=1, Probability=0.0060\n",
            "  ('trust', 'show'): Count=1, Probability=0.0060\n",
            "  ('show', 'faint'): Count=1, Probability=0.0060\n",
            "  ('faint', 'hearted'): Count=1, Probability=0.0060\n",
            "  ('hearted', 'timid'): Count=1, Probability=0.0060\n",
            "  ('timid', 'show'): Count=1, Probability=0.0060\n",
            "  ('show', 'pulls'): Count=1, Probability=0.0060\n",
            "  ('pulls', 'punches'): Count=1, Probability=0.0060\n",
            "  ('punches', 'regards'): Count=1, Probability=0.0060\n",
            "  ('regards', 'drugs'): Count=1, Probability=0.0060\n",
            "  ('drugs', 'sex'): Count=1, Probability=0.0060\n",
            "  ('sex', 'violence'): Count=1, Probability=0.0060\n",
            "  ('violence', 'hardcore'): Count=1, Probability=0.0060\n",
            "  ('hardcore', 'classic'): Count=1, Probability=0.0060\n",
            "  ('classic', 'use'): Count=1, Probability=0.0060\n",
            "  ('use', 'wordit'): Count=1, Probability=0.0060\n",
            "  ('wordit', 'called'): Count=1, Probability=0.0060\n",
            "  ('called', 'oz'): Count=1, Probability=0.0060\n",
            "  ('oz', 'nickname'): Count=1, Probability=0.0060\n",
            "  ('nickname', 'given'): Count=1, Probability=0.0060\n",
            "  ('given', 'oswald'): Count=1, Probability=0.0060\n",
            "  ('oswald', 'maximum'): Count=1, Probability=0.0060\n",
            "  ('maximum', 'security'): Count=1, Probability=0.0060\n",
            "  ('security', 'state'): Count=1, Probability=0.0060\n",
            "  ('state', 'penitentary'): Count=1, Probability=0.0060\n",
            "  ('penitentary', 'focuses'): Count=1, Probability=0.0060\n",
            "  ('focuses', 'mainly'): Count=1, Probability=0.0060\n",
            "  ('mainly', 'emerald'): Count=1, Probability=0.0060\n",
            "  ('emerald', 'city'): Count=1, Probability=0.0060\n",
            "  ('city', 'experimental'): Count=1, Probability=0.0060\n",
            "  ('experimental', 'section'): Count=1, Probability=0.0060\n",
            "  ('section', 'prison'): Count=1, Probability=0.0060\n",
            "  ('prison', 'cells'): Count=1, Probability=0.0060\n",
            "  ('cells', 'glass'): Count=1, Probability=0.0060\n",
            "  ('glass', 'fronts'): Count=1, Probability=0.0060\n",
            "  ('fronts', 'face'): Count=1, Probability=0.0060\n",
            "  ('face', 'inwards'): Count=1, Probability=0.0060\n",
            "  ('inwards', 'privacy'): Count=1, Probability=0.0060\n",
            "  ('privacy', 'high'): Count=1, Probability=0.0060\n",
            "  ('high', 'agenda'): Count=1, Probability=0.0060\n",
            "  ('agenda', 'em'): Count=1, Probability=0.0060\n",
            "  ('em', 'city'): Count=1, Probability=0.0060\n",
            "  ('city', 'home'): Count=1, Probability=0.0060\n",
            "  ('home', 'manyaryans'): Count=1, Probability=0.0060\n",
            "  ('manyaryans', 'muslims'): Count=1, Probability=0.0060\n",
            "  ('muslims', 'gangstas'): Count=1, Probability=0.0060\n",
            "  ('gangstas', 'latinos'): Count=1, Probability=0.0060\n",
            "  ('latinos', 'christians'): Count=1, Probability=0.0060\n",
            "  ('christians', 'italians'): Count=1, Probability=0.0060\n",
            "  ('italians', 'irish'): Count=1, Probability=0.0060\n",
            "  ('irish', 'moreso'): Count=1, Probability=0.0060\n",
            "  ('moreso', 'scuffles'): Count=1, Probability=0.0060\n",
            "  ('scuffles', 'death'): Count=1, Probability=0.0060\n",
            "  ('death', 'stares'): Count=1, Probability=0.0060\n",
            "  ('stares', 'dodgy'): Count=1, Probability=0.0060\n",
            "  ('dodgy', 'dealings'): Count=1, Probability=0.0060\n",
            "  ('dealings', 'shady'): Count=1, Probability=0.0060\n",
            "  ('shady', 'agreements'): Count=1, Probability=0.0060\n",
            "  ('agreements', 'never'): Count=1, Probability=0.0060\n",
            "  ('never', 'far'): Count=1, Probability=0.0060\n",
            "  ('far', 'awayi'): Count=1, Probability=0.0060\n",
            "  ('awayi', 'would'): Count=1, Probability=0.0060\n",
            "  ('would', 'say'): Count=1, Probability=0.0060\n",
            "  ('say', 'main'): Count=1, Probability=0.0060\n",
            "  ('main', 'appeal'): Count=1, Probability=0.0060\n",
            "  ('appeal', 'show'): Count=1, Probability=0.0060\n",
            "  ('show', 'due'): Count=1, Probability=0.0060\n",
            "  ('due', 'fact'): Count=1, Probability=0.0060\n",
            "  ('fact', 'goes'): Count=1, Probability=0.0060\n",
            "  ('goes', 'shows'): Count=1, Probability=0.0060\n",
            "  ('shows', 'wouldnt'): Count=1, Probability=0.0060\n",
            "  ('wouldnt', 'dare'): Count=1, Probability=0.0060\n",
            "  ('dare', 'forget'): Count=1, Probability=0.0060\n",
            "  ('forget', 'pretty'): Count=1, Probability=0.0060\n",
            "  ('pretty', 'pictures'): Count=1, Probability=0.0060\n",
            "  ('pictures', 'painted'): Count=1, Probability=0.0060\n",
            "  ('painted', 'mainstream'): Count=1, Probability=0.0060\n",
            "  ('mainstream', 'audiences'): Count=1, Probability=0.0060\n",
            "  ('audiences', 'forget'): Count=1, Probability=0.0060\n",
            "  ('forget', 'charm'): Count=1, Probability=0.0060\n",
            "  ('charm', 'forget'): Count=1, Probability=0.0060\n",
            "  ('forget', 'romanceoz'): Count=1, Probability=0.0060\n",
            "  ('romanceoz', 'doesnt'): Count=1, Probability=0.0060\n",
            "  ('doesnt', 'mess'): Count=1, Probability=0.0060\n",
            "  ('mess', 'around'): Count=1, Probability=0.0060\n",
            "  ('around', 'first'): Count=1, Probability=0.0060\n",
            "  ('first', 'episode'): Count=1, Probability=0.0060\n",
            "  ('episode', 'ever'): Count=1, Probability=0.0060\n",
            "  ('ever', 'saw'): Count=1, Probability=0.0060\n",
            "  ('saw', 'struck'): Count=1, Probability=0.0060\n",
            "  ('struck', 'nasty'): Count=1, Probability=0.0060\n",
            "  ('nasty', 'surreal'): Count=1, Probability=0.0060\n",
            "  ('surreal', 'couldnt'): Count=1, Probability=0.0060\n",
            "  ('couldnt', 'say'): Count=1, Probability=0.0060\n",
            "  ('say', 'ready'): Count=1, Probability=0.0060\n",
            "  ('ready', 'watched'): Count=1, Probability=0.0060\n",
            "  ('watched', 'developed'): Count=1, Probability=0.0060\n",
            "  ('developed', 'taste'): Count=1, Probability=0.0060\n",
            "  ('taste', 'oz'): Count=1, Probability=0.0060\n",
            "  ('oz', 'got'): Count=1, Probability=0.0060\n",
            "  ('got', 'accustomed'): Count=1, Probability=0.0060\n",
            "  ('accustomed', 'high'): Count=1, Probability=0.0060\n",
            "  ('high', 'levels'): Count=1, Probability=0.0060\n",
            "  ('levels', 'graphic'): Count=1, Probability=0.0060\n",
            "  ('graphic', 'violence'): Count=1, Probability=0.0060\n",
            "  ('violence', 'violence'): Count=1, Probability=0.0060\n",
            "  ('violence', 'injustice'): Count=1, Probability=0.0060\n",
            "  ('injustice', 'crooked'): Count=1, Probability=0.0060\n",
            "  ('crooked', 'guards'): Count=1, Probability=0.0060\n",
            "  ('guards', 'wholl'): Count=1, Probability=0.0060\n",
            "  ('wholl', 'sold'): Count=1, Probability=0.0060\n",
            "  ('sold', 'nickel'): Count=1, Probability=0.0060\n",
            "  ('nickel', 'inmates'): Count=1, Probability=0.0060\n",
            "  ('inmates', 'wholl'): Count=1, Probability=0.0060\n",
            "  ('wholl', 'kill'): Count=1, Probability=0.0060\n",
            "  ('kill', 'order'): Count=1, Probability=0.0060\n",
            "  ('order', 'get'): Count=1, Probability=0.0060\n",
            "  ('get', 'away'): Count=1, Probability=0.0060\n",
            "  ('away', 'well'): Count=1, Probability=0.0060\n",
            "  ('well', 'mannered'): Count=1, Probability=0.0060\n",
            "  ('mannered', 'middle'): Count=1, Probability=0.0060\n",
            "  ('middle', 'class'): Count=1, Probability=0.0060\n",
            "  ('class', 'inmates'): Count=1, Probability=0.0060\n",
            "  ('inmates', 'turned'): Count=1, Probability=0.0060\n",
            "  ('turned', 'prison'): Count=1, Probability=0.0060\n",
            "  ('prison', 'bitches'): Count=1, Probability=0.0060\n",
            "  ('bitches', 'due'): Count=1, Probability=0.0060\n",
            "  ('due', 'lack'): Count=1, Probability=0.0060\n",
            "  ('lack', 'street'): Count=1, Probability=0.0060\n",
            "  ('street', 'skills'): Count=1, Probability=0.0060\n",
            "  ('skills', 'prison'): Count=1, Probability=0.0060\n",
            "  ('prison', 'experience'): Count=1, Probability=0.0060\n",
            "  ('experience', 'watching'): Count=1, Probability=0.0060\n",
            "  ('watching', 'oz'): Count=1, Probability=0.0060\n",
            "  ('oz', 'may'): Count=1, Probability=0.0060\n",
            "  ('may', 'become'): Count=1, Probability=0.0060\n",
            "  ('become', 'comfortable'): Count=1, Probability=0.0060\n",
            "  ('comfortable', 'uncomfortable'): Count=1, Probability=0.0060\n",
            "  ('uncomfortable', 'viewingthats'): Count=1, Probability=0.0060\n",
            "  ('viewingthats', 'get'): Count=1, Probability=0.0060\n",
            "  ('get', 'touch'): Count=1, Probability=0.0060\n",
            "  ('touch', 'darker'): Count=1, Probability=0.0060\n",
            "  ('darker', 'side'): Count=1, Probability=0.0060\n",
            "\n",
            "Trigrams:\n",
            "  ('one', 'reviewers', 'mentioned'): Count=1, Probability=0.0060\n",
            "  ('reviewers', 'mentioned', 'watching'): Count=1, Probability=0.0060\n",
            "  ('mentioned', 'watching', '1'): Count=1, Probability=0.0060\n",
            "  ('watching', '1', 'oz'): Count=1, Probability=0.0060\n",
            "  ('1', 'oz', 'episode'): Count=1, Probability=0.0060\n",
            "  ('oz', 'episode', 'youll'): Count=1, Probability=0.0060\n",
            "  ('episode', 'youll', 'hooked'): Count=1, Probability=0.0060\n",
            "  ('youll', 'hooked', 'right'): Count=1, Probability=0.0060\n",
            "  ('hooked', 'right', 'exactly'): Count=1, Probability=0.0060\n",
            "  ('right', 'exactly', 'happened'): Count=1, Probability=0.0060\n",
            "  ('exactly', 'happened', 'methe'): Count=1, Probability=0.0060\n",
            "  ('happened', 'methe', 'first'): Count=1, Probability=0.0060\n",
            "  ('methe', 'first', 'thing'): Count=1, Probability=0.0060\n",
            "  ('first', 'thing', 'struck'): Count=1, Probability=0.0060\n",
            "  ('thing', 'struck', 'oz'): Count=1, Probability=0.0060\n",
            "  ('struck', 'oz', 'brutality'): Count=1, Probability=0.0060\n",
            "  ('oz', 'brutality', 'unflinching'): Count=1, Probability=0.0060\n",
            "  ('brutality', 'unflinching', 'scenes'): Count=1, Probability=0.0060\n",
            "  ('unflinching', 'scenes', 'violence'): Count=1, Probability=0.0060\n",
            "  ('scenes', 'violence', 'set'): Count=1, Probability=0.0060\n",
            "  ('violence', 'set', 'right'): Count=1, Probability=0.0060\n",
            "  ('set', 'right', 'word'): Count=1, Probability=0.0060\n",
            "  ('right', 'word', 'go'): Count=1, Probability=0.0060\n",
            "  ('word', 'go', 'trust'): Count=1, Probability=0.0060\n",
            "  ('go', 'trust', 'show'): Count=1, Probability=0.0060\n",
            "  ('trust', 'show', 'faint'): Count=1, Probability=0.0060\n",
            "  ('show', 'faint', 'hearted'): Count=1, Probability=0.0060\n",
            "  ('faint', 'hearted', 'timid'): Count=1, Probability=0.0060\n",
            "  ('hearted', 'timid', 'show'): Count=1, Probability=0.0060\n",
            "  ('timid', 'show', 'pulls'): Count=1, Probability=0.0060\n",
            "  ('show', 'pulls', 'punches'): Count=1, Probability=0.0060\n",
            "  ('pulls', 'punches', 'regards'): Count=1, Probability=0.0060\n",
            "  ('punches', 'regards', 'drugs'): Count=1, Probability=0.0060\n",
            "  ('regards', 'drugs', 'sex'): Count=1, Probability=0.0060\n",
            "  ('drugs', 'sex', 'violence'): Count=1, Probability=0.0060\n",
            "  ('sex', 'violence', 'hardcore'): Count=1, Probability=0.0060\n",
            "  ('violence', 'hardcore', 'classic'): Count=1, Probability=0.0060\n",
            "  ('hardcore', 'classic', 'use'): Count=1, Probability=0.0060\n",
            "  ('classic', 'use', 'wordit'): Count=1, Probability=0.0060\n",
            "  ('use', 'wordit', 'called'): Count=1, Probability=0.0060\n",
            "  ('wordit', 'called', 'oz'): Count=1, Probability=0.0060\n",
            "  ('called', 'oz', 'nickname'): Count=1, Probability=0.0060\n",
            "  ('oz', 'nickname', 'given'): Count=1, Probability=0.0060\n",
            "  ('nickname', 'given', 'oswald'): Count=1, Probability=0.0060\n",
            "  ('given', 'oswald', 'maximum'): Count=1, Probability=0.0060\n",
            "  ('oswald', 'maximum', 'security'): Count=1, Probability=0.0060\n",
            "  ('maximum', 'security', 'state'): Count=1, Probability=0.0060\n",
            "  ('security', 'state', 'penitentary'): Count=1, Probability=0.0060\n",
            "  ('state', 'penitentary', 'focuses'): Count=1, Probability=0.0060\n",
            "  ('penitentary', 'focuses', 'mainly'): Count=1, Probability=0.0060\n",
            "  ('focuses', 'mainly', 'emerald'): Count=1, Probability=0.0060\n",
            "  ('mainly', 'emerald', 'city'): Count=1, Probability=0.0060\n",
            "  ('emerald', 'city', 'experimental'): Count=1, Probability=0.0060\n",
            "  ('city', 'experimental', 'section'): Count=1, Probability=0.0060\n",
            "  ('experimental', 'section', 'prison'): Count=1, Probability=0.0060\n",
            "  ('section', 'prison', 'cells'): Count=1, Probability=0.0060\n",
            "  ('prison', 'cells', 'glass'): Count=1, Probability=0.0060\n",
            "  ('cells', 'glass', 'fronts'): Count=1, Probability=0.0060\n",
            "  ('glass', 'fronts', 'face'): Count=1, Probability=0.0060\n",
            "  ('fronts', 'face', 'inwards'): Count=1, Probability=0.0060\n",
            "  ('face', 'inwards', 'privacy'): Count=1, Probability=0.0060\n",
            "  ('inwards', 'privacy', 'high'): Count=1, Probability=0.0060\n",
            "  ('privacy', 'high', 'agenda'): Count=1, Probability=0.0060\n",
            "  ('high', 'agenda', 'em'): Count=1, Probability=0.0060\n",
            "  ('agenda', 'em', 'city'): Count=1, Probability=0.0060\n",
            "  ('em', 'city', 'home'): Count=1, Probability=0.0060\n",
            "  ('city', 'home', 'manyaryans'): Count=1, Probability=0.0060\n",
            "  ('home', 'manyaryans', 'muslims'): Count=1, Probability=0.0060\n",
            "  ('manyaryans', 'muslims', 'gangstas'): Count=1, Probability=0.0060\n",
            "  ('muslims', 'gangstas', 'latinos'): Count=1, Probability=0.0060\n",
            "  ('gangstas', 'latinos', 'christians'): Count=1, Probability=0.0060\n",
            "  ('latinos', 'christians', 'italians'): Count=1, Probability=0.0060\n",
            "  ('christians', 'italians', 'irish'): Count=1, Probability=0.0060\n",
            "  ('italians', 'irish', 'moreso'): Count=1, Probability=0.0060\n",
            "  ('irish', 'moreso', 'scuffles'): Count=1, Probability=0.0060\n",
            "  ('moreso', 'scuffles', 'death'): Count=1, Probability=0.0060\n",
            "  ('scuffles', 'death', 'stares'): Count=1, Probability=0.0060\n",
            "  ('death', 'stares', 'dodgy'): Count=1, Probability=0.0060\n",
            "  ('stares', 'dodgy', 'dealings'): Count=1, Probability=0.0060\n",
            "  ('dodgy', 'dealings', 'shady'): Count=1, Probability=0.0060\n",
            "  ('dealings', 'shady', 'agreements'): Count=1, Probability=0.0060\n",
            "  ('shady', 'agreements', 'never'): Count=1, Probability=0.0060\n",
            "  ('agreements', 'never', 'far'): Count=1, Probability=0.0060\n",
            "  ('never', 'far', 'awayi'): Count=1, Probability=0.0060\n",
            "  ('far', 'awayi', 'would'): Count=1, Probability=0.0060\n",
            "  ('awayi', 'would', 'say'): Count=1, Probability=0.0060\n",
            "  ('would', 'say', 'main'): Count=1, Probability=0.0060\n",
            "  ('say', 'main', 'appeal'): Count=1, Probability=0.0060\n",
            "  ('main', 'appeal', 'show'): Count=1, Probability=0.0060\n",
            "  ('appeal', 'show', 'due'): Count=1, Probability=0.0060\n",
            "  ('show', 'due', 'fact'): Count=1, Probability=0.0060\n",
            "  ('due', 'fact', 'goes'): Count=1, Probability=0.0060\n",
            "  ('fact', 'goes', 'shows'): Count=1, Probability=0.0060\n",
            "  ('goes', 'shows', 'wouldnt'): Count=1, Probability=0.0060\n",
            "  ('shows', 'wouldnt', 'dare'): Count=1, Probability=0.0060\n",
            "  ('wouldnt', 'dare', 'forget'): Count=1, Probability=0.0060\n",
            "  ('dare', 'forget', 'pretty'): Count=1, Probability=0.0060\n",
            "  ('forget', 'pretty', 'pictures'): Count=1, Probability=0.0060\n",
            "  ('pretty', 'pictures', 'painted'): Count=1, Probability=0.0060\n",
            "  ('pictures', 'painted', 'mainstream'): Count=1, Probability=0.0060\n",
            "  ('painted', 'mainstream', 'audiences'): Count=1, Probability=0.0060\n",
            "  ('mainstream', 'audiences', 'forget'): Count=1, Probability=0.0060\n",
            "  ('audiences', 'forget', 'charm'): Count=1, Probability=0.0060\n",
            "  ('forget', 'charm', 'forget'): Count=1, Probability=0.0060\n",
            "  ('charm', 'forget', 'romanceoz'): Count=1, Probability=0.0060\n",
            "  ('forget', 'romanceoz', 'doesnt'): Count=1, Probability=0.0060\n",
            "  ('romanceoz', 'doesnt', 'mess'): Count=1, Probability=0.0060\n",
            "  ('doesnt', 'mess', 'around'): Count=1, Probability=0.0060\n",
            "  ('mess', 'around', 'first'): Count=1, Probability=0.0060\n",
            "  ('around', 'first', 'episode'): Count=1, Probability=0.0060\n",
            "  ('first', 'episode', 'ever'): Count=1, Probability=0.0060\n",
            "  ('episode', 'ever', 'saw'): Count=1, Probability=0.0060\n",
            "  ('ever', 'saw', 'struck'): Count=1, Probability=0.0060\n",
            "  ('saw', 'struck', 'nasty'): Count=1, Probability=0.0060\n",
            "  ('struck', 'nasty', 'surreal'): Count=1, Probability=0.0060\n",
            "  ('nasty', 'surreal', 'couldnt'): Count=1, Probability=0.0060\n",
            "  ('surreal', 'couldnt', 'say'): Count=1, Probability=0.0060\n",
            "  ('couldnt', 'say', 'ready'): Count=1, Probability=0.0060\n",
            "  ('say', 'ready', 'watched'): Count=1, Probability=0.0060\n",
            "  ('ready', 'watched', 'developed'): Count=1, Probability=0.0060\n",
            "  ('watched', 'developed', 'taste'): Count=1, Probability=0.0060\n",
            "  ('developed', 'taste', 'oz'): Count=1, Probability=0.0060\n",
            "  ('taste', 'oz', 'got'): Count=1, Probability=0.0060\n",
            "  ('oz', 'got', 'accustomed'): Count=1, Probability=0.0060\n",
            "  ('got', 'accustomed', 'high'): Count=1, Probability=0.0060\n",
            "  ('accustomed', 'high', 'levels'): Count=1, Probability=0.0060\n",
            "  ('high', 'levels', 'graphic'): Count=1, Probability=0.0060\n",
            "  ('levels', 'graphic', 'violence'): Count=1, Probability=0.0060\n",
            "  ('graphic', 'violence', 'violence'): Count=1, Probability=0.0060\n",
            "  ('violence', 'violence', 'injustice'): Count=1, Probability=0.0060\n",
            "  ('violence', 'injustice', 'crooked'): Count=1, Probability=0.0060\n",
            "  ('injustice', 'crooked', 'guards'): Count=1, Probability=0.0060\n",
            "  ('crooked', 'guards', 'wholl'): Count=1, Probability=0.0060\n",
            "  ('guards', 'wholl', 'sold'): Count=1, Probability=0.0060\n",
            "  ('wholl', 'sold', 'nickel'): Count=1, Probability=0.0060\n",
            "  ('sold', 'nickel', 'inmates'): Count=1, Probability=0.0060\n",
            "  ('nickel', 'inmates', 'wholl'): Count=1, Probability=0.0060\n",
            "  ('inmates', 'wholl', 'kill'): Count=1, Probability=0.0060\n",
            "  ('wholl', 'kill', 'order'): Count=1, Probability=0.0060\n",
            "  ('kill', 'order', 'get'): Count=1, Probability=0.0060\n",
            "  ('order', 'get', 'away'): Count=1, Probability=0.0060\n",
            "  ('get', 'away', 'well'): Count=1, Probability=0.0060\n",
            "  ('away', 'well', 'mannered'): Count=1, Probability=0.0060\n",
            "  ('well', 'mannered', 'middle'): Count=1, Probability=0.0060\n",
            "  ('mannered', 'middle', 'class'): Count=1, Probability=0.0060\n",
            "  ('middle', 'class', 'inmates'): Count=1, Probability=0.0060\n",
            "  ('class', 'inmates', 'turned'): Count=1, Probability=0.0060\n",
            "  ('inmates', 'turned', 'prison'): Count=1, Probability=0.0060\n",
            "  ('turned', 'prison', 'bitches'): Count=1, Probability=0.0060\n",
            "  ('prison', 'bitches', 'due'): Count=1, Probability=0.0060\n",
            "  ('bitches', 'due', 'lack'): Count=1, Probability=0.0060\n",
            "  ('due', 'lack', 'street'): Count=1, Probability=0.0060\n",
            "  ('lack', 'street', 'skills'): Count=1, Probability=0.0060\n",
            "  ('street', 'skills', 'prison'): Count=1, Probability=0.0060\n",
            "  ('skills', 'prison', 'experience'): Count=1, Probability=0.0060\n",
            "  ('prison', 'experience', 'watching'): Count=1, Probability=0.0060\n",
            "  ('experience', 'watching', 'oz'): Count=1, Probability=0.0060\n",
            "  ('watching', 'oz', 'may'): Count=1, Probability=0.0060\n",
            "  ('oz', 'may', 'become'): Count=1, Probability=0.0060\n",
            "  ('may', 'become', 'comfortable'): Count=1, Probability=0.0060\n",
            "  ('become', 'comfortable', 'uncomfortable'): Count=1, Probability=0.0060\n",
            "  ('comfortable', 'uncomfortable', 'viewingthats'): Count=1, Probability=0.0060\n",
            "  ('uncomfortable', 'viewingthats', 'get'): Count=1, Probability=0.0060\n",
            "  ('viewingthats', 'get', 'touch'): Count=1, Probability=0.0060\n",
            "  ('get', 'touch', 'darker'): Count=1, Probability=0.0060\n",
            "  ('touch', 'darker', 'side'): Count=1, Probability=0.0060\n",
            "\n",
            "N-gram probability calculation complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Explanation & Analysis\n",
        " Understanding N-gram Representations\n",
        "\n",
        "Unigrams treat each word independently and represent text as individual tokens.\n",
        "This approach is simple but ignores word order and context.\n",
        "\n",
        "Bigrams capture pairs of consecutive words, allowing the model to learn short-range dependencies such as negations (not good).\n",
        "\n",
        "Trigrams model longer word sequences and preserve more contextual information, but they increase dimensionality and sparsity.\n",
        "\n",
        "As the value of n increases, contextual understanding improves, but the number of unique N-grams grows significantly, making the representation more sparse and computationally expensive."
      ],
      "metadata": {
        "id": "s4P-wSzWfKFc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Vectorization Techniques\n",
        "Objective\n",
        "\n",
        "Convert textual reviews into numerical form so they can be used by machine learning models.\n",
        "\n",
        "**Approach**\n",
        "\n",
        "1. Text data cannot be directly processed by machine learning algorithms.\n",
        "\n",
        "2. Therefore, vectorization techniques are used to represent text as numerical vectors.\n",
        "\n",
        "3. In this project, Bag of Words using CountVectorizer is applied."
      ],
      "metadata": {
        "id": "3bUUKUb3T6qO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Initialize CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit and transform cleaned text data\n",
        "X_vectors = vectorizer.fit_transform(df['clean_review'])\n",
        "\n",
        "# Display matrix shape\n",
        "print(\"Vectorized data shape:\", X_vectors.shape)\n",
        "print(\"Total documents:\", X_vectors.shape[0])\n",
        "print(\"Vocabulary size:\", X_vectors.shape[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTw15Xtxf-Uc",
        "outputId": "277d443d-f7f9-4705-d45e-09151df73c94"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vectorized data shape: (50000, 221431)\n",
            "Total documents: 50000\n",
            "Vocabulary size: 221431\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert one review into dense format for display\n",
        "sample_vector = X_vectors[0].toarray()\n",
        "print(\"Sample vector (first 15 values):\", sample_vector[0][:15])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEjzB_nfUkh-",
        "outputId": "b3c6835b-254b-4c49-c6d7-33f2551eef71"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample vector (first 15 values): [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_words = vectorizer.get_feature_names_out()\n",
        "print(\"Sample vocabulary words:\", vocab_words[100:110])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8-NjyR1XcEO",
        "outputId": "f7b83f73-770c-485b-dbee-e7bea7f76387"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample vocabulary words: ['1000000000' '10000000000' '1000000000000' '1000000000000010'\n",
            " '1000000000000010000000000000' '100000dm' '100001' '10002000' '10005000'\n",
            " '1000lb']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4. Text Classification Model (Naive Bayes)\n",
        "Model Used\n",
        "\n",
        "Multinomial Naive Bayes, suitable for count-based text features."
      ],
      "metadata": {
        "id": "RUkwuxqtXh9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n"
      ],
      "metadata": {
        "id": "oplrxRY5XoU5"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preparing target labels\n",
        "y = df['sentiment']   # already encoded\n",
        "X = X_vectors\n"
      ],
      "metadata": {
        "id": "vWO4NF2yhi1O"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test , train and split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=1\n",
        ")\n",
        "\n",
        "print(\"Training samples:\", X_train.shape)\n",
        "print(\"Testing samples:\", X_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEETvhGUX1yt",
        "outputId": "3866b2fe-1837-47d3-d97d-37e9eb7655d8"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: (37500, 221431)\n",
            "Testing samples: (12500, 221431)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# traing model\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "mocURrrtX7-6",
        "outputId": "6fc95c33-f766-40d9-8a2b-017f1b8c8f07"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-1 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-1 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-1 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-1 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-1 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>MultinomialNB</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.naive_bayes.MultinomialNB.html\">?<span>Documentation for MultinomialNB</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>MultinomialNB()</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prediction\n",
        "y_predictions = nb_model.predict(X_test)\n",
        ""
      ],
      "metadata": {
        "id": "Ct2UCXHMX_li"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5. Model Evaluation\n",
        "Evaluation Metrics Used\n",
        "\n",
        "Accuracy\n",
        "\n",
        "Precision\n",
        "\n",
        "Recall\n",
        "\n",
        "F1-Score\n",
        "\n",
        "Confusion Matrix"
      ],
      "metadata": {
        "id": "Sl7rj3qZYT4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n"
      ],
      "metadata": {
        "id": "r1IPnRk8YF_v"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Metric Result"
      ],
      "metadata": {
        "id": "kIloIMJ9iG4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy:\", accuracy_score(y_test, y_predictions))\n",
        "print(\"Precision:\", precision_score(y_test, y_predictions))\n",
        "print(\"Recall:\", recall_score(y_test, y_predictions))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_predictions))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybFu8qWxiFH0",
        "outputId": "fa88ae16-3ca4-42e3-e026-023f9d217016"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8636\n",
            "Precision: 0.8748539963290506\n",
            "Recall: 0.845918038076799\n",
            "F1 Score: 0.8601427282421459\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Confusion Matrix"
      ],
      "metadata": {
        "id": "MkKWFxt0iUhv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_predictions))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ba6BT7oiiRXq",
        "outputId": "f1420868-946f-4cf0-dbb6-02f356aba3ca"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            " [[5552  750]\n",
            " [ 955 5243]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Sample Predictions"
      ],
      "metadata": {
        "id": "24OBcdRyiaYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "    print(\"\\nReview:\", df['review'].iloc[y_test.index[i]][:100])\n",
        "    print(\"Actual:\", \"Positive\" if y_test.iloc[i] == 1 else \"Negative\")\n",
        "    print(\"Predicted:\", \"Positive\" if y_predictions[i] == 1 else \"Negative\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLdvkMkEiZ5s",
        "outputId": "e2dd3767-3c22-4873-ee88-7305012cdc0d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Review: With No Dead Heroes you get stupid lines like that as this woefully abysmal action flick needs to be\n",
            "Actual: Negative\n",
            "Predicted: Negative\n",
            "\n",
            "Review: I thought maybe... maybe this could be good. An early appearance by the Re-Animator (Jeffery Combs);\n",
            "Actual: Negative\n",
            "Predicted: Negative\n",
            "\n",
            "Review: An elite American military team which of course happens to include two good looking women and a guy \n",
            "Actual: Negative\n",
            "Predicted: Negative\n",
            "\n",
            "Review: Ridiculous horror film about a wealthy man (John Carradine) dying and leaving everything to his four\n",
            "Actual: Negative\n",
            "Predicted: Negative\n",
            "\n",
            "Review: Well, if you are one of those Katana's film-nuts (just like me) you sure will appreciate this metaph\n",
            "Actual: Positive\n",
            "Predicted: Positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Unigrams vs Bigrams Comparison"
      ],
      "metadata": {
        "id": "yiu39fHLii9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer_bi = CountVectorizer(ngram_range=(1,2))\n",
        "X_bi = vectorizer_bi.fit_transform(df['clean_review'])\n",
        "\n",
        "X_train_bi, X_test_bi, y_train_bi, y_test_bi = train_test_split(\n",
        "    X_bi, y, test_size=0.25, random_state=1\n",
        ")\n",
        "\n",
        "nb_bi = MultinomialNB()\n",
        "nb_bi.fit(X_train_bi, y_train_bi)\n",
        "\n",
        "y_pred_bi = nb_bi.predict(X_test_bi)\n",
        "\n",
        "print(\"Bigram Accuracy:\", accuracy_score(y_test_bi, y_pred_bi))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgqTtIb9inyJ",
        "outputId": "7ff24a20-7497-417c-ad36-9227ee242c19"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigram Accuracy: 0.88672\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**6. Reflection**\n",
        "Short Answers\n",
        "\n",
        "###Effect of stemming/lemmatization:\n",
        "Reduced vocabulary size and improved consistency by grouping similar word forms.\n",
        "\n",
        "###Did n-grams improve performance?\n",
        "Bigrams slightly improved results by capturing short phrases like not good.\n",
        "\n",
        "###BoW vs CountVectorizer:\n",
        "Both are similar, but CountVectorizer provides better control and preprocessing options.\n",
        "\n",
        "###Why Naive Bayes works well for text?\n",
        "It handles high-dimensional sparse data efficiently and performs well with word frequencies.\n",
        "\n",
        "###Future improvements:\n",
        "\n",
        "1. use TF-IDF\n",
        "\n",
        "2. Apply word embeddings (Word2Vec, GloVe)\n",
        "\n",
        "3. Experiment with deep learning models (LSTM, BERT)"
      ],
      "metadata": {
        "id": "nSRVPS37jGBS"
      }
    }
  ]
}